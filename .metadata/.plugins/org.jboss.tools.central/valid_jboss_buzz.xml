<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">How to configure SSL/HTTPS on WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-security/complete-tutorial-for-configuring-ssl-https-on-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-security/complete-tutorial-for-configuring-ssl-https-on-wildfly/</id><updated>2023-01-18T10:51:49Z</updated><content type="html">This is a complete tutorial about configuring SSL/HTTPS support for JBoss EAP / WildFly application server. Generally speaking, to configure SSL/HTTPS you can either use the pure JSSE implementation (and the keytool utility) or a native implementation such as OpenSSL. We will cover at first the JSSE implementation with keytool. Later we will show how ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to access WildFly Admin Console</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-configuration/how-to-access-wildfly-admin-console/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-configuration/how-to-access-wildfly-admin-console/</id><updated>2023-01-18T10:01:44Z</updated><content type="html">This short tutorial will teach you how to access WildFly Admin Console also known as WildFly Management Console. WildFly Admin Console Default URL The default URL for WildFly Management Console is http://localhost:9990. Upon installation, you cannot connect to WildFly Management Console because there is no default Admin user for the Management Console. To add a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Solving java.lang.OutOfMemoryError: Metaspace error</title><link rel="alternate" href="http://www.mastertheboss.com/java/solving-java-lang-outofmemoryerror-metaspace-error/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/solving-java-lang-outofmemoryerror-metaspace-error/</id><updated>2023-01-18T08:22:55Z</updated><content type="html">The java.lang.OutOfMemoryError: Metaspace indicates that the amount of native memory allocated for Java class metadata is exausted. Let’s how this issue can be solved in standalone applications and cloud applications. In general terms, the more classes you are loading into the JVM, the more memory will be consumed by the metaspace. In Java 8 and ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How we expanded GCC value range propagation to floats</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/01/18/how-we-expanded-gcc-value-range-propagation-floats" /><author><name>Aldy Hernandez</name></author><id>e7d46cbf-eb97-4f23-9ac6-cf48a163e976</id><updated>2023-01-18T07:00:00Z</updated><published>2023-01-18T07:00:00Z</published><summary type="html">&lt;p&gt;Value range propagation (VRP) is an optimization tool used in compilers. The article &lt;a href="https://developers.redhat.com/blog/2021/04/28/value-range-propagation-in-gcc-with-project-ranger"&gt;Value range propagation in GCC with Project Ranger&lt;/a&gt; describes how this optimization works and how the GCC team implements it for &lt;a href="https://developers.redhat.com/topics/c"&gt;C and C++&lt;/a&gt; programs. This article will explain how we expand VRP beyond integers and pointers to other data types, particularly floating-point numbers.&lt;/p&gt; &lt;h2&gt;Expanding data types in range tracking&lt;/h2&gt; &lt;p&gt;A generic, type-agnostic way to keep track of ranges was part of our original goals for the Ranger project. At first, VRP tracked just integers and pointers, and we also wanted to apply it to floats, strings, and other data types.&lt;/p&gt; &lt;p&gt;We spent a good chunk of the GCC 13 release ridding Ranger of any dependencies on integers baked in the original VRP implementation. We made Ranger work with a generic &lt;code&gt;vrange&lt;/code&gt; class instead of the &lt;code&gt;irange&lt;/code&gt; that was specific to integers and pointers. Then we developed an infrastructure to declare typeless temporaries that could be used in intermediate computations within Ranger. When all was said and done, we had a core that could provide VRP, jump threading, and other range-aware optimizations on anything we could express in terms of &lt;code&gt;vrange&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;For the curious, &lt;code&gt;vrange&lt;/code&gt; is nothing more than an abstract class that describes operations on properties (ranges, in our case). It is basically a class to make it easy for Ranger to do operations on sets (union, intersect, etc.):&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;class vrange { public: virtual void set (tree, tree, value_range_kind = VR_RANGE); virtual tree type () const; virtual bool supports_type_p (const_tree type) const; virtual void set_varying (tree type); virtual void set_undefined (); virtual bool union_ (const vrange &amp;); virtual bool intersect (const vrange &amp;); virtual bool singleton_p (tree *result = NULL) const; virtual bool contains_p (tree cst) const; virtual bool zero_p () const; virtual bool nonzero_p () const; virtual void set_nonzero (tree type); virtual void set_zero (tree type); virtual void set_nonnegative (tree type); virtual bool fits_p (const vrange &amp;r) const; … … };&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Current support for floating-point numbers&lt;/h2&gt; &lt;p&gt;Once the &lt;code&gt;vrange&lt;/code&gt; class was in place, our next step was to provide a barebones implementation of &lt;code&gt;frange&lt;/code&gt; (floating point range) that would give us enough tools to fold conditionals and perhaps keep track of "not a number" values (NaNs). We accomplished this task in the simplest way we could find, through a class that keeps track of endpoints and whether a positive or negative NaN is possible:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;class frange : public vrange { … … private: tree m_type; REAL_VALUE_TYPE m_min; REAL_VALUE_TYPE m_max; bool m_pos_nan; bool m_neg_nan; };&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After much more work than anticipated (because floats are painfully hard, and things never go according to plan), here are a handful of things we now support in GCC 13:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Folding of symbolic relational operators:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;if (x &gt; y) { if (x == y) link_error (); }&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Propagation of NaNs and infinities:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;if (x &gt; y) { // x is not a NAN // y is not a NAN // x is not -INF // y is not +INF }&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Intervals for ranges:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;if (x &gt;= 5.0) { // x is not a NAN // x is [5.0, +INF] } else { // x is [-INF, 4.99999952316] U [NAN] }&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Signed zeros:&lt;/p&gt; &lt;pre&gt; &lt;code class="cpp"&gt;if (x == 0.0) { if (__builtin_sign (x)) y = x; // y = -0.0 else z = x; // z = +0.0 }&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A handful of operations, including +, -, *, /, negate, abs, relational operators, unordered relational operators, etc.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Even with this initial implementation, we have been able to close quite a few long-standing PRs (bug reports and enhancement requests) related to floats, including PR24021 (&lt;a href="https://gcc.gnu.org/bugzilla/show_bug.cgi?id=24021"&gt;VRP does not work with floating points&lt;/a&gt;), which has been with us for 17 years!&lt;/p&gt; &lt;h2&gt;Using optimized processor instructions &lt;/h2&gt; &lt;p&gt;The goal of this work is to provide an infrastructure to increasingly flesh out floating point operations that can help us do better range propagation, as well as aid other optimizations that generate better code. For example, some architectures provide a cheap &lt;code&gt;sqrt&lt;/code&gt; instruction that works on positive numbers but not on NaNs. With floating point ranges, the instruction selection pass might determine that the argument to sqrt is neither a negative value nor a NaN and can replace an expensive sqrt instruction with a cheaper one.&lt;/p&gt; &lt;p&gt;We also hope to make use of this work with glibc in the next release to provide entry points into libm when the operands like &lt;code&gt;sin()&lt;/code&gt; or &lt;code&gt;cos()&lt;/code&gt; functions are known to be in a certain range and void of NaNs and INFs. This improvement will allow us to generate faster code because the compiler can choose cheaper versions of said functions.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/01/18/how-we-expanded-gcc-value-range-propagation-floats" title="How we expanded GCC value range propagation to floats"&gt;How we expanded GCC value range propagation to floats&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Aldy Hernandez</dc:creator><dc:date>2023-01-18T07:00:00Z</dc:date></entry><entry><title type="html">Smarter Decision Tables Generation through Data Types Constraints</title><link rel="alternate" href="https://blog.kie.org/2023/01/automatically-generating-decision-tables-in-dmn.html" /><author><name>Jozef Marko</name></author><id>https://blog.kie.org/2023/01/automatically-generating-decision-tables-in-dmn.html</id><updated>2023-01-17T19:57:51Z</updated><content type="html">The Decision Model and Notation (DMN) is an effective, yet standardized, tool for designing complex decisions, in special decision tables. Even though you, DMN developer, already knew about the existence of this capability, here’s something you didn’t know: there is a new DMN Decision Table column data type constraint enhancement in the DMN Editor. Let’s check all about this new feature (), available in the KIE Business Automation Tools starting version , that can facilitate and speed up the design and authoring of decision tables. In this article, you will understand when and how to use this new DMN feature part of the Business Automation Tools under the . As well described in the documentation, according to DMN specification, we can use data types to determine the structure of the data that we want to use within an associated decision table, column, or field in the boxed expression. Other than the default DMN data types (such as String, Number, Boolean) we can create custom data types to specify additional fields and constraints. &gt; TIP: SHOULD I USE CONSTRAINTS IN MY MODELS? &gt; &gt; We do not apply constraints in real world so often. We have infinite amount of &gt; colors, places, vendors, etc.. However as DMN developers, we restrict the &gt; domain usually. Here we will develop DMN traffic lights example where we will &gt; check just red and green color. Yes, we know that some countries may use also &gt; orange one and some other traffic lights may use totally different colors. &gt; However it is just a proof that as DMN developers we need constraints. Now, looking back to the way we used to work with decision tables, there was the need to manually synchronize every decision table column with the constraints of data types being used. Fortunately, from now on: the Decisions Editor is responsible for synchronization automatically during the first generation of the decision table. From this point on, we have a great feature at hand to help us out, but it is important to keep in mind that the editor does not synchronize all your following changes in Data Types and in the Existing Decision Table. In other words, the synchronization will help you out the first time you generate a decision table.  Now, let’s see how we can use these new decision table capabilities. REQUIREMENTS In order to try out this enhancement, you will need one of the environments from the table below: VS Code IDE v9.9+ with The online editor Now, let’s understand a bit more about the usage of constraints and how to set them up.  HOW TO DEFINE DATA TYPE CONSTRAINTS So we need data type constraints. In the DMN Editor, we have a separate Data Types tab that allows us to manage the data types.  You can find detailed information and how-to-use guides available not only in our other , but in the community as well. To get started, design a new data type tColor like below: If we look more precisely, it contains just two colors (red and green) that are allowed values of tColor data type. Now that we have defined our data type, we can proceed to use DMN to design the Traffic Lights decisions. The Traffic Lights decisions will be represented in what is called a Decision Requirement Diagram (DRD). DESIGNING THE DECISION TABLE In regards to the logic of the decision table we will author, we can consider as an input the color of the traffic light, which is either red or green:  When this decision is a red color, we can not walk.When there is a green color, we can walk.  Next, DMN allows us to define additional data types with a constraint – tMeaning. Let’s see how to do it below: So we have the data type tMeaning and two constraints that define which are the allowed values, in this case, “do not walk” and “walk”. Probably you already have an idea of how it is related to “red” and “green” colors. Moving forward, let’s look at the DRD.  The main diagram will contain just two nodes that can be configured as follows: The first node is an Input Data node. The Input Data node represents the color of the traffic light.We can set its type reference as tColor, according to its business meaning.Add a new Decision Node that will help us figure out which is the meaning of the received input traffic light color.Connect the Input Data and the Decision node.The Decision Node: represents the meaning. This is where our custom type tMeaning becomes useful! Now, we should set the decision type reference as the type tMeaning. * At this point, we have two connected nodes with types set and we miss the last, but not least, thing, and it is a decision table with column data type constraint. Now comes the fun part which we can see the new capability in action. AUTOMATICALLY DECISION TABLE GENERATION The DMN Editor is now able to pre-generate each column of the decision table based on the information requirements of each node. By defining the constraints in our custom data type, and configuring the table to this data type, we allowed the editor to automatically abstract which information could be part of the decision table, saving us some extra time! The only task is then to simply define two rows of the decision table.  See the demanded result below: In case you are still asking yourself, “So where is the promised enhancement?”, we want you to notice that until now, we have done a lot of manual configurations. However, if you check column header details, you will realize, the column data type constraint was automatically set. Simply click “Traffic Light Color” or “Human Meaning” cells and check the properties panel properly. The data type constraint proper configuration in the decision table’s header details is essential for an efficient decision table gap analysis. FEATURE EXTRA POINTS: GAP ANALYSIS Let’s say during the design phase, we forgot to handle one of the traffic light colors in our decision table. Thanks to column data type constraint, the tooling is now able to detect we are missing the validation of one data. This happens in the background, and as you are authoring your decision, the analysis of the model is running. Luckily, we can be assured that the tool has us covered, as it will display a message warning about exactly which light traffic color we forget to handle in our decision table. The full sample, including the complete diagram source code, is available as a gist . The post appeared first on .</content><dc:creator>Jozef Marko</dc:creator></entry><entry><title type="html">Using REST Services to upload and download files</title><link rel="alternate" href="http://www.mastertheboss.com/jboss-frameworks/resteasy/using-rest-services-to-manage-download-and-upload-of-files/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jboss-frameworks/resteasy/using-rest-services-to-manage-download-and-upload-of-files/</id><updated>2023-01-17T18:07:32Z</updated><content type="html">This REST Service tutorial is a quick guide for handling files upload and download using REST Services. We will create and test a Rest Service to upload and download files using JAX-RS API. Finally, we will show how to build a JUnit 5 Test to test files uploading using RESTEasy Client API. Uploading and Downloading ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">How to use Log4j2 in your WildFly applications</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-use-log4j2-in-your-wildfly-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-use-log4j2-in-your-wildfly-applications/</id><updated>2023-01-17T15:16:53Z</updated><content type="html">Log4j2 is the latest major release of the popular Logging Framework. In this tutorial we will learn how to include Log4j2 configuration file and use it in your deployments running on WildFly. Overview of Log4j2 Log4j2 is a powerful logging library, developed by Apache, that provides advanced features such as: Asynchronous Logging: Log4j2 allows for ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Deploy dashboards to OpenShift</title><link rel="alternate" href="https://blog.kie.org/2023/01/deploy-dashboards-to-openshift.html" /><author><name>Guilherme Caponetto</name></author><id>https://blog.kie.org/2023/01/deploy-dashboards-to-openshift.html</id><updated>2023-01-17T13:59:09Z</updated><content type="html">Starting from the KIE Tools release, the enables users to easily deploy their dashboards to OpenShift. Let’s check out how to set up and use this new feature in this post! Photo by on  INTRODUCTION makes it easy for users to create rich dashboards to accommodate their data in meaningful visualizations. Just like for serverless workflow files, the capability to deploy dashboards to OpenShift has also been added to . It allows users not only to design their dashboards but also to make them available to be shared with others through their OpenShift instance. The deploy button will be available on the toolbar once a dashboard is open in the editor. Users can either deploy only the dashboard they are currently visualizing or the entire workspace, which may have more dashboards and related data files to be deployed. Behind the scenes, the files are placed along with a that includes a web application and the viewer. On the OpenShift side, all necessary resources are created on top of Knative Serving and a builder is triggered to prepare the deployment. The deployment will scale up and be ready to be accessed once the building process is completed. After one minute of inactivity, the deployment will scale down to zero pods. In case of the deployment is accessed again, it will automatically scale up. This is Knative making sure you save resources of your OpenShift instance. SET UP CONNECTIONS There are two mandatory configurations that must be done in order to use the deploy operation: (1) run the KIE Sandbox Extended Services; (2) set up the OpenShift instance information. KIE SANDBOX EXTENDED SERVICES It is mandatory to run the KIE Sandbox Extended Services since it bridges the requests between the and OpenShift. If the KIE Sandbox Extended Services is not running, go to and click on the Settings button ️located in the top right corner of the page (⚙). Access the KIE Sandbox Extended Services tab and click on "Click to setup". Then follow the instructions for your Operating system to get the KIE Sandbox Extended Services up and running. KIE Sandbox Extended Services tab – not connected KIE Sandbox Extended Services tab — connected OPENSHIFT INSTANCE INFORMATION If the OpenShift instance information is not configured yet, go to and click on the Settings button ️located in the top right corner of the page (⚙). Access the OpenShift tab and fill in the fields with your OpenShift instance information (namespace/project, host, and token). Note: The personal access token from OpenShift expires every 24 hours. Note: You can use the , which gives you 30 days of free access to a shared OpenShift and Kubernetes cluster. You will only need your phone to confirm the activation of your instance. OpenShift tab — connected DEPLOY THE DASHBOARD Go to and open a dashboard. The sample will be used as an example for this post. Once the dashboard is ready to be deployed, click on "Try on OpenShift" located in the toolbar and "Deploy". A popup will be open for you to confirm this operation. Confirmation popup for deploying dashboards If you simply confirm, only the dashboard you are currently visualizing will be deployed. If you want to deploy the entire workspace, check the "Deploy workspace" box. After the confirmation, all resources will be created in your OpenShift instance and a builder will be started. You can follow the progress of the deployment operation either on or in your OpenShift instance. Once the building process is completed, the deployment will be scaled up and ready to be shared. Now, let’s take a look at this feature in action. And that’s all for today. Thanks for reading! 😃 The post appeared first on .</content><dc:creator>Guilherme Caponetto</dc:creator></entry><entry><title type="html">How to configure SLF4J in WildFly applications</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/how-to-configure-slf4j-in-wildfly-applications/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/how-to-configure-slf4j-in-wildfly-applications/</id><updated>2023-01-17T13:15:59Z</updated><content type="html">In this tutorial, we will discuss how to use Simple Logging Facade for Java (SLF4J) with Wildfly application server. SLF4J is a logging facade that provides a unified interface for various logging frameworks, such as log4j, java.util.logging, and Logback. It allows for the decoupling of the application code from the underlying logging framework, making it ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Getting Started with Simple Logging Facade (SLF4J)</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-log/getting-started-with-simple-logging-facade-slf4j/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-log/getting-started-with-simple-logging-facade-slf4j/</id><updated>2023-01-17T12:47:53Z</updated><content type="html">Simple Logging Facade for Java (SLF4J) is a logging facade that provides a unified interface for various logging frameworks, such as log4j, java.util.logging, and Logback. It allows for the decoupling of the application code from the underlying logging framework, making it easier to change the logging framework without modifying the application code. Here is a ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry></feed>
